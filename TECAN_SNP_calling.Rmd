---
title: "_Ascochyta rabiei_ SNP calling from WGRS pipeline"
author: "Ido Bar & Hayley Wilson"
date: "`r format(Sys.Date(), '%d %B %Y')`"
always_allow_html: yes
output: 

    bookdown::html_document2:
      includes:
       in_header: style/header.html
       after_body: style/external-links-js.html
      df_print: paged
      theme: 
        version: 5
        bootswatch: simplex #sandstone #zephyr # yeti # united
        # primary: "#6CC3D4"
      highlight: tango
      css: "style/style.css"
      toc: true
      toc_float: true
      toc_depth: 4
  #    highlight: pygments
      number_sections: false
      code_folding: hide
#      keep_md: true
bibliography: style/Fungal_genomes.bib
csl: style/springer-basic-improved-author-date-with-italic-et-al-period.csl
---

```{r setup, include=FALSE}
devtools::source_gist("7f63547158ecdbacf31b54a58af0d1cc", filename = "util.R")
knitr::opts_chunk$set(list(echo = TRUE, eval=FALSE, message=FALSE))
# options(width = 180)
# Sys.setenv(JAVA_HOME = "C:/Program Files/Java/jre1.8.0_341")

# remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"), INSTALL_opts = "--no-multiarch")
 # CRAN settings
chooseCRANmirror(ind=1)
options(repos = getOption("repos")["CRAN"]) # fix the annoying "unable to access index..."
req_pacs <- c("tidyverse", 'readxl', 'scales', 'janitor', 'paletteer', 'VariantAnnotation', "cn.mops", "htmltools", "bookdown",
              'ggrepel', 'here',  'DT', 'plotly', 'mmod', 'poppr', 'circlize', "downloadthis", "tabulizer", "htmltab/htmltab", 'Mikata-Project/ggthemr')

pak::pak(req_pacs)
pacman::p_load(basename(req_pacs), install = FALSE, update =FALSE)
```


## Experimental Design

DNA was extracted from XXX isolates of *Ascochyta rabiei* (collected in 2024-25) and prepared for low-pass whole-genome resequencing (WGRS) at the State Agricultural Biotechnology Centre ([SABC](https://www.murdoch.edu.au/research/sabc)) at Murdoch University (led by Prof. Rajeev Varshney). DNA libraries were prepared and sequenced on an MGI DNBSEQ-T7, producing 150 bp paired-end reads (run name CAGRF23101109).

## Aims

**UPDATE**    

- Identify strain-unique variants to develop detection methods  
- Associate aggressiveness with specific variants  


## Methods

### Overview of Analysis Pipeline

1.  Data pre-processing:  
    a.  Quality check  
    b.  Adaptor trimming  
    c.  Post-trim quality check  
2.  Mapping reads to a reference genome  
3.  Reads deduplication and read group addition  
4.  Variant calling and filtration  
5.  Population genetics analysis (clustering)  

Sequencing data processing, mapping and variant calling were performed on the *QCIF Bunya* (using Slurm scheduler, see [documentation](https://github.com/UQ-RCC/hpc-docs/tree/main)) and in [GalaxyAU](https://usegalaxy.org.au). 


### Data pre-processing

Install needed software in a `conda` environment on the HPC cluster (we will install a [Miniforge distribution](https://github.com/conda-forge/miniforge), which has `mamba` already installed - see [mamba docs](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html)).

```{bash setup-conda}
# download miniforge conda
wget "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
bash Miniforge3-$(uname)-$(uname -m).sh
# accept defaults and let conda initialise
# initialise conda
source ~/.bashrc
# add channels and set priorities
conda config --add channels conda-forge
conda config --append channels bioconda

# install extra packages to the base environment
mamba install -n base libgcc gnutls libuuid readline cmake git tmux libgfortran parallel mamba gawk pigz rename genozip autoconf sshpass gh
# install snippy (need to fix internet connection to gowonda2 - use patched netcheck in ~/bin)
# source ~/.proxy
CONDA_NAME=genomics
mamba create -n $CONDA_NAME snippy sra-tools bcbio-gff libgd xorg-libxpm \
                libpng libjpeg-turbo jpeg snpsift rename biobambam bwa-mem2 sambamba \
                libtiff genozip parallel qualimap multiqc bbmap fastp freebayes bedops 
# Clean extra space
# conda update -n base conda
conda clean -y --all

# Prepare a general array Slurm script
echo '#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --output=%x.%A.%a.log'"
#SBATCH --account=a_agri_genomics
#SBATCH --partition=general

set -Eeo pipefail
source ~/.bashrc
conda activate \$CONDA_NAME
cd \$SLURM_SUBMIT_DIR
gawk -v ARRAY_IND=\$SLURM_ARRAY_TASK_ID 'NR==ARRAY_IND' \$CMDS_FILE | bash" > ~/bin/array.slurm

echo '#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --output=%x.%j.log'"
#SBATCH --account=a_agri_genomics
#SBATCH --partition=general

set -Eeo pipefail
source ~/.bashrc
conda activate \$CONDA_NAME
cd \$SLURM_SUBMIT_DIR
bash \$CMDS_FILE" > ~/bin/serial_jobs_run.slurm

# Prepare a parallel jobs Slurm script
echo '
cat $CMDS_FILE | parallel' | cat <(head -n -1  ~/bin/serial_jobs_run.slurm) - > ~/bin/parallel_jobs_run.slurm
```

Download the raw `.fastq.gz` and reference genome files from CloudStor. 

```{bash retrieve-files}
# Download genome files
REF_DIR=$HOME/data/reference_genomes/ArME14_v2_CCDM # on Bunya HPC
rclone copy -P --include "ArME14.fasta.gz*" --include "ArME14-genes.gff3*" GURS_shared:GRI2007-001RTX/ME14_reference_and_annotations/ArME14_v2_CCDM $REF_DIR
# create a working directory
WORK_DIR=/scratch/project/adna/A_rabiei
mkdir -p $WORK_DIR
cd $WORK_DIR
# download TECAN read files from GU Research Space
rclone copy -P GURS_shared:GRI2007-001RTX/A_rabiei_TECAN/AGRF_CAGRF23101109_22GHFLLT3 ./AGRF_CAGRF23101109_22GHFLLT3
```

Create a folder for our processing (`$RUN_DIR`) and prepare the reference genome (create accessory index files).

```{bash prep-genomes}
# start an interactive job on Bunya
alias start_interactive_job='salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=10 --mem=50G --job-name=interactive --time=05:00:00 --partition=general --account=a_agri_genomics srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l'
WORK_DIR=/scratch/project/adna/A_rabiei/
CONDA_NAME=genomics
conda activate $CONDA_NAME
# BBMAP_REF=$(find $CONDA_PREFIX -wholename "*resources/adapters.fa")
# Prepare the commands
RUN_DIR=$WORK_DIR/A_rabiei_TECAN_2022
mkdir -p $RUN_DIR/ref_genome && cd $RUN_DIR
REF_DIR=$HOME/data/reference_genomes/ArME14_v2_CCDM # New published genome from CCDM on Bunya HPC
# REF_DIR=$HOME/data/reference_genomes # on Awoonga HPC
# GENOME="$REF_DIR/ArME14"
GENOME="$RUN_DIR/ref_genome/ArME14_v2_CCDM"
# ln -s $REF_DIR/Ascochyta_rabiei_ArME14.scaffolds.fa $GENOME.fa 
# ln -s $REF_DIR/ArME14_all_annotations.updated.gff3 $GENOME.gff3 
pigz -cd  $REF_DIR/ArME14.fasta.gz > $GENOME.fa
pigz -cd  $REF_DIR/ArME14-genes.gff3.gz > $GENOME.gff3
gff2bed < $GENOME.gff3 > $GENOME.bed
bwa-mem2 index $GENOME.fa 
samtools faidx $GENOME.fa

```

### Mapping to the updated reference genome

<!-- Note that `bwa-mem2` reported an error with some files not being ordered/paired properly, so we processed them first with `repair.sh` command from BBtools v39.01 (see [reported issue](https://github.com/lh3/bwa/issues/228#issuecomment-534769169) and [repair guide](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/repair-guide/)). The repaired read pairs were then processed by `fastp` v0.23.4 to remove sequencing adapters and low-quality bases and reads [@chenUltrafastOnepassFASTQ2023]. -->

<!-- Prepare and run the pipeline to process the reads -->

<!-- ```{bash repair-reads} -->
<!-- FQ_DIR=$WORK_DIR/AGRF_CAGRF23101109_22GHFLLT3 -->
<!-- mkdir -p $RUN_DIR/fixed_reads/QC -->

<!-- # 1 pair of reads example -->
<!-- # repair.sh in=$FQ_DIR/Ar22829_22GHFLLT3_TCCAACTG_L001_R1.fastq.gz out=$RUN_DIR/fixed_reads/Ar22829_22GHFLLT3_TCCAACTG_L001_R#.fixed.fastq.gz repair ow zl=7 -->
<!-- # Ar22829_22GHFLLT3_TCCAACTG_L001_R1.fastq.gz -->
<!-- NCORES=16 -->
<!-- MEM=96 -->
<!-- WALLTIME="10:00:00" -->
<!-- JOBNAME=process_reads -->
<!-- CONDA_NAME=genomics -->
<!-- # Create the commands to repair all read pairs. -->
<!-- ls -1 $FQ_DIR/*_R1.fastq.gz | parallel -k --dry-run --rpl "{infile} s:_R1:_R#:; uq()" --rpl "{sample} s:.+/(.+?)_22GHFLLT3_[ACGT\-]+_L00[0-4]_.+:\1:"  "repair.sh in={infile} out=$RUN_DIR/fixed_reads/{sample}_R#.fixed.fastq.gz repair ow zl=7 threads=\$[SLURM_CPUS_PER_TASK - 2]; fastp -i $RUN_DIR/fixed_reads/{sample}_R1.fixed.fastq.gz -I $RUN_DIR/fixed_reads/{sample}_R2.fixed.fastq.gz --detect_adapter_for_pe -c -l 30 -p -w \$SLURM_CPUS_PER_TASK -z 7 -o $RUN_DIR/fixed_reads/QC/{sample}_R1.fixed.trimmed.fastq.gz -O $RUN_DIR/fixed_reads/QC/{sample}_R2.fixed.trimmed.fastq.gz -j $RUN_DIR/fixed_reads/QC/{sample}.fastp.json -h $RUN_DIR/fixed_reads/QC/{sample}.fastp.html" > $JOBNAME.cmds -->

<!-- # submit it as a Slurm job -->
<!-- echo '#!/bin/bash --login -->
<!-- #SBATCH --nodes=1 -->
<!-- #SBATCH --ntasks=1 -->
<!-- #SBATCH --output=%x.%j.log'" -->
<!-- #SBATCH --job-name=$JOBNAME -->
<!-- #SBATCH --cpus-per-task=$NCORES -->
<!-- #SBATCH --mem=${MEM}G -->
<!-- #SBATCH --time=$WALLTIME -->
<!-- #SBATCH --account=a_agri_genomics -->
<!-- #SBATCH --partition=general -->

<!-- set -Eeo pipefail -->
<!-- source ~/.bashrc -->
<!-- conda activate $CONDA_NAME -->
<!-- cd \$SLURM_SUBMIT_DIR -->
<!-- srun bash $JOBNAME.cmds" > $JOBNAME.slurm -->
<!-- # submit the job  -->
<!-- sbatch $JOBNAME.slurm -->

<!-- ``` -->

Reads were mapped to the *A. rabiei* reference genome assembled and annotated by the CCDM, Curtin University, (NCBI accession [GCF_004011695.2](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_004011695.2/)) using bwa-mem2 v2.2.1 [@vasimuddinEfficientArchitectureAwareAcceleration2019]. The alignment files were then coordinate-sorted and PCR duplicates were marked using the `bamsormadup` command from BioBamBam2 v2.0.183 [@tischlerBiobambamToolsRead2014].\
Mapping quality was assessed with Qualimap v.2.2.2-dev [@okonechnikovQualimapAdvancedMultisample2016] and consolidated along with quality-trimming measures into a single, interactive report for each batch using MultiQC v1.21 [@ewelsMultiQCSummarizeAnalysis2016]. Samples with less than 20% mapping and x15 coverage were removed from the rest of the analysis (see details in the [MultiQC report](raw_data/QC_11_10_2021_multiqc_report.html)).

```{bash process-align-reads}
# setup workspace
CONDA_NAME=genomics                                                                           REF_DIR="/scratch/project/adna/A_rabiei/A_rabiei_TECAN_2022/ref_genome"
GENOME="$REF_DIR/ArME14_v2_CCDM"
WORK_DIR="/scratch/project/adna/A_rabiei/Murdoch_WGRS"
FQ_DIR="$WORK_DIR/fungal"
DATE=$(date +%d_%m_%Y)
RUN_DIR=$WORK_DIR/SNP_calling_${DATE}
mkdir -p $RUN_DIR/aligned_reads && mkdir -p $RUN_DIR/trimmed_reads/QC && cd $RUN_DIR
NCORES=12
MEM=64
WALLTIME="2:00:00"
JOBNAME="process_reads"
# create commands
find $FQ_DIR/*.R1.fq.gz | parallel -k --dry-run --rpl "{file2} s:.R1:.R2:; uq()" --rpl "{sample} s:.+\/(.+?).R1.fq.gz:\1:"  "fastp -i {}
-I {file2} --detect_adapter_for_pe -c -l 30 -p -w \$SLURM_CPUS_PER_TASK -z 7 -o $RUN_DIR/trimmed_reads/{sample}_R1.trimmed.fastq.gz -O $RUN_DIR/
trimmed_reads/{sample}_R2.trimmed.fastq.gz -j $RUN_DIR/trimmed_reads/QC/{sample}.fastp.json" > $JOBNAME.cmds

# submit to the cluster
sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l)%10 --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=
ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm

JOBNAME="bwa-mem-align"
NCORES=12
MEM=96
WALLTIME="1:00:00"

# Create the bwa-mem2 commands to align all read pairs.
find $RUN_DIR/trimmed_reads -name "*_R1.trimmed.fastq.gz" | parallel -k --dry-run --rpl "{file2} s:_R1:_R2:" --rpl "{sample} s:.+\/(.+?)_
R1.+:\1:"  "ALIGN_DIR=$RUN_DIR/aligned_reads && bwa-mem2 mem -R \"@RG\tID:{1 sample}\tSM:{1 sample}\tLB:{1 sample}\tPU:E250038400\tPL:DNBseq_T7\
tCN:MU_SABC\" -t \$[SLURM_CPUS_PER_TASK - 2] $GENOME.fa {1} {file2} | bamsormadup tmpfile=/scratch/project/adna/tmp/bamsormadup_\$(hostname) inp
utformat=sam threads=\$[SLURM_CPUS_PER_TASK - 2] > \$ALIGN_DIR/{sample}.dedup.rg.csorted.bam; unset DISPLAY; qualimap bamqc -bam \$ALIGN_DIR/{sa
mple}.dedup.rg.csorted.bam --java-mem-size=32G -c -gff $GENOME.bed -outdir \$ALIGN_DIR/{sample}_bamqc"  > $RUN_DIR/$JOBNAME.cmds

# submit to the cluster
sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l)%10 --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=
ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm
```

### Calling variants (using Freebayes)

We used Freebayes *v1.3.5* [@garrisonHaplotypebasedVariantDetection2012a] to assign variant probability scores and call variants. Notice that we used diploid mode (`-p 2`) despite _A. rabiei_ being haploid, to be able to identify loci wrongly called as heterozygotes which indicates a mapping issue. We also added the flag `--genotype-qualities` to be able to filter the resulting vcf file based on genotype qualities (`GQ`). 

```{bash call-variants-murdoch-wgrs}
WORK_DIR=/scratch/project/adna/A_rabiei
RUN_DIR=$WORK_DIR/A_rabiei_TECAN_2022
GENOME="$WORK_DIR/A_rabiei_TECAN_2022/ref_genome/ArME14_v2_CCDM"
CONDA_NAME=genomics
NCORES=16
MEM=96
WALLTIME="50:00:00"
BAM_DIR=$RUN_DIR/aligned_reads

# Distributed freebayes (each node runs freebayes-parallel on one contig)
# download script
aria2c -c -x5 -d ~/bin https://raw.githubusercontent.com/freebayes/freebayes/master/scripts/split_ref_by_bai_datasize.py 
chmod +x ~/bin/split_ref_by_bai_datasize.py
mamba install -y -n $CONDA_NAME numpy scipy

# start_interactive_job
conda activate $CONDA_NAME
# fix library dependencies
find $CONDA_PREFIX -name "libtabixpp.so*" | parallel ln -s {} {.}.0
# ln -s $CONDA_PREFIX/lib/libtabixpp.so.1 $CONDA_PREFIX/lib/libtabixpp.so.0
# split each contig/chromosome to smaller 1e6 bits
# prepare BAM files
JOBNAME="prep_bams"
# submit it as a Slurm job
echo "ls -1 $BAM_DIR/*.rg.csorted.bam | parallel -j2 sambamba index {} -t  \$[SLURM_CPUS_PER_TASK/2]
~/bin/split_ref_by_bai_datasize.py -s 1e6 -r $GENOME.fa.fai $(ls -1S $BAM_DIR/*.dedup.rg.csorted.bam | tail -n1) > $RUN_DIR/ArME14_target_1e6_reg
ions_chr.tsv" > $JOBNAME.cmds
# submit the job 
sbatch -a --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=
ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/serial_jobs_run.slurm

JOBNAME="Murdoch_FB_diploid"
RUN_DIR=$WORK_DIR/SNP_calling_24_01_2025
PLOIDY=2
MIN_DP=7
# prepare commands
BAM_FILES=$( find $BAM_DIR -maxdepth 1 -name "*.rg.csorted.bam" -size +1M  | xargs )
cut -f1 $GENOME.fa.fai | parallel --dry-run "freebayes-parallel <(grep '{}' $RUN_DIR/ArME14_target_1e6_regions_chr.tsv | gawk '{printf \"%s:%s-%s\n\", \$1, \$2, \$3}') \$SLURM_CPUS_PER_TASK -f $GENOME.fa --genotype-qualities -g 100000 -C $MIN_DP -p $PLOIDY $BAM_FILES > $RUN_DIR/FB_array_output/{}.combined.vcf" > $RUN_DIR/$JOBNAME.cmds
mkdir -p $RUN_DIR/FB_array_output
# exit interactive job
# send to the cluster
sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l)%10 --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=${RUN_DIR}/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME  ~/bin/array.slurm 

# run in diploid mode
JOBNAME="FB_diploid"
RUN_DIR=$WORK_DIR/A_rabiei_TECAN_2022/$JOBNAME
PLOIDY=2
mkdir -p $RUN_DIR/FB_array_output
cd $RUN_DIR

# prepare commands
BAM_FILES=$( find $BAM_DIR -maxdepth 1 -name "*.rg.csorted.bam" -size +1M  | xargs )
cut -f1 $GENOME.fa.fai | parallel --dry-run "freebayes-parallel <(grep '{}' $WORK_DIR/A_rabiei_TECAN_2022/ArME14_target_1e6_regions_chr.tsv | gawk '{printf \"%s:%s-%s\n\", \$1, \$2, \$3}') \$SLURM_CPUS_PER_TASK  -f $GENOME.fa -g 50000 --genotype-qualities -C $MIN_DP -p $PLOIDY $BAM_FILES > $RUN_DIR/FB_array_output/{}.combined.vcf" > $RUN_DIR/$JOBNAME.cmds

mkdir -p $RUN_DIR/FB_array_output
# exit interactive job
# send to the cluster
sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l)%10 --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm 

# merge variants
ls -1 $RUN_DIR/FB_array_output/ArME14_ctg_*.combined.vcf | parallel --dry-run "printf \"{}\t%s\t%s\t%s\t%s\n\" \$(cat {} | grep -c -v '^#
') \$(cat {} | SnpSift filter \"( GEN[?].DP > $MIN_DP ) & ( GEN[?].GT != './.' )\" | gawk '{if (\$0 ~ /^#/ || (length(\$4)==1 && length(\$5)==1)
); print \$0}' | grep -c -v '^#') \$(cat {} | SnpSift filter \"( GEN[?].DP > $MIN_DP ) & ( GEN[?].GT != './.' ) & ( QUAL > 20 )\" | gawk '{if (\
$0 ~ /^#/ || (length(\$4)==1 && length(\$5)==1)); print \$0}' | grep -c -v '^#') \$(cat {} | SnpSift filter \"( GEN[?].DP > $MIN_DP ) & ( GEN[?]
.GT != './.' ) & ( QUAL > 30 )\" | gawk '{if (\$0 ~ /^#/ || (length(\$4)==1 && length(\$5)==1)); print \$0}' | grep -c -v '^#') > {}.stats" > ${
RUN_DIR}/freebayes-stats.cmds
JOBNAME=freebayes-merge
echo "cat ${RUN_DIR}/freebayes-stats.cmds | parallel && cat $RUN_DIR/FB_array_output/ArME14_ctg_*.combined.vcf | vcffirstheader | vcfstre
amsort -w 1000 | vcfuniq > A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid.vcf && 
bgzip A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid.vcf && tabix A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid.vcf.gz" > $JOBNAME.
cmds
# submit job to cluster
sbatch -a --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=
ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/serial_jobs_run.slurm
# combaine stats
cat <(printf "file\ttotal_snps\tDP${MIN_DP}_filtered_snps\tQUAL20_filtered_snps\tQUAL30_filtered_snps\n") <(cat $RUN_DIR/FB_array_output/
ArME14_ctg_*.vcf.stats) > A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid_vcf_stats_$(date +%d_%m_%Y).txt


```

 
#### Create and filter final variant file
The variants that were called individually in each chromosome were merged into a single `vcf` file, which was gzipped, indexed. Variants were filtered using a combination of commands from SnpSift *v5.1d* [@rudenUsingDrosophilaMelanogaster2012], BCFtools *v1.17* [@danecekTwelveYearsSAMtools2021; @liStatisticalFrameworkSNP2011] and VCFtools *v0.1.16* [@danecekVariantCallFormat2011a], based on their total loci depth and quality, keeping only bi-allelic polymorphic SNP loci with an average loci depth of 10 and not more than 100,000 reads covering the locus (`10<DP<100000`, based on EDA). In addition, each isolate's genotype call was reset (recoded as missing, or `./.`) if it had read depth (`DP<7`) or called as a heterozygote. Variant statistics were generated by BCFtools pre and post filter.

```{bash vcf_filter}
CONDA_NAME="genomics"
conda activate $CONDA_NAME
# Recode genotypes as missing if below a certain threshold, such as genotyping quality or depth (GQ:DP)  
# filter only polymorphic bi-allelic SNPs, using QUAL>20, 7<DP<100000

# filter Freebayes variants with SnpSift and vcftools (wipe any heterozygote genotype with DP<7 with bcftools)
QUAL=20
MAX_DP=100000
MIN_DP=10
IND_DP=7
bcftools filter -S . -e 'GT=="het" | FMT/DP<7' A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid.vcf.gz -O v | SnpSift filter "( QUAL>
=$QUAL ) & ( DP<$MAX_DP ) & ( DP>$MIN_DP ) & ( countRef()>=1 & countVariant()>=1 )" | vcftools --vcf -  --recode --recode-INFO-all --minQ $QUAL
--max-missing 0.75 --remove-indels --out A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid.Q$QUAL.GT75.noRep.noHet.poly

bgzip A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid.Q$QUAL.GT75.noRep.noHet.poly.recode.vcf && tabix A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid.Q$QUAL.GT75.noRep.noHet.poly.recode.vcf.gz
# generate stats NEED COMPLETING!
JOBNAME="bcftools_stats"
WALLTIME=2:00:00
MEM=32
NCORES=8
find . -name "*.vcf.gz" | parallel --dry-run "bcftools stats -s - {} > {.}.bcfstats.txt" > $JOBNAME.cmds
# send to the cluster
sbatch --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/parallel_jobs_run.slurm 
```

An in-house R script (`estimate_error_rates_vcf_files.R`) was used to estimate the error rates based on the presence of duplicated samples.

#### MutilQC

```{bash multiqc}
NCORES=8
MEM=32
WALLTIME="10:00:00"
JOBNAME="Multiqc_Murdoch_WGRS"
# multiqc report
MULTIQC_JOB=QC_$(date +%d_%m_%Y)
# submit it as a Slurm job
echo "multiqc --interactive --force -i $MULTIQC_JOB -o $MULTIQC_JOB ." > $JOBNAME.cmds
# submit the job 
sbatch --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/serial_jobs_run.slurm 
# Done!

# Copy html files to SharePoint
rclone copy -P $RUN_DIR GRDC_rabiei:General/Projects/Hayley_PhD/Genetics/Murdoch_WGRS/SNP_calling_24_01_2025
# Copy html files to SharePoint
rclone copy -P --ignore-checksum --ignore-size --include "**/*.html" $RUN_DIR GRDC_rabiei:General/Projects/Hayley_PhD/Genetics/Murdoch_WGRS/SNP_calling_24_01_2025
```



### Comparison of Alignment and SNP calling pipelines

In order to select the optimal SNP calling pipeline that will strike a good balance between sensitive SNP discovery and the lowest error rates (as measured by heterozygote calls and inconsistencies between technical replicates [AR0039, AR0052, AR0242] ).
We will test different alignment tools and parameters (`BWA-MEM2`, `Bowtie2`) with different variant callers (`GATK`, `FreeBayes`, `Clair3`) and a complete `Nextflow` pipeline implementing those (`Sarek`).

We will test the following alignment options:  
1. `BWA-MEM2` with `-B` ranging from `4:6` and `-O 6/7/8` (which includes default `-B 4 -O 6`).  
2. `Bowtie2` in `local` alignment mode: `--local-fast`, `--local-sensitive` (default), `--local--very-sensitive`.  
3. Same as above, but in `global` alignment mode: `--global-sensitive`, etc.  



```{bash alignment-comparison}
# install tools
CONDA_NAME=genomics
mamba install -y -n $CONDA_NAME bwa-mem2 bowtie2 biobambam sambamba qualimap multiqc fastp
# assign variables
REF_DIR="/scratch/project/adna/A_rabiei/A_rabiei_TECAN_2022/ref_genome"
GENOME="$REF_DIR/ArME14_v2_CCDM"
# build index for bowtie2
# conda activate $CONDA_NAME
# or run with a container (need to login to a compute node)
start_debug_interactive_job
apptainer exec $NXF_APPTAINER_CACHEDIR/depot.galaxyproject.org-singularity-bowtie2-2.4.4--py39hbb4e92a_0.img bowtie2-build $GENOME.fa $GENOME
WORK_DIR="/scratch/project/adna/A_rabiei"
FQ_DIR="$WORK_DIR/A_rabiei_WGS_2021"
# create a working folder
RUN_DIR=$WORK_DIR/SNP_calling_comparison
mkdir -p $RUN_DIR/aligned_reads && mkdir -p $RUN_DIR/trimmed_reads/QC && cd $RUN_DIR


NCORES=12
MEM=64
WALLTIME="5:00:00"
JOBNAME=process_reads
CONDA_NAME=genomics
# Create the commands to repair all read pairs.
find $FQ_DIR/*_R1.fastq.gz | egrep "AR0039|AR0052|AR0242" | parallel -k --dry-run --rpl "{infile} s:_R1:_R#:; uq()" --rpl "{sample} s:.+\/(.+?)_H3HGFDSX2_[ACGT\-]+_L00[0-4]_.+:\1:"  "repair.sh in={infile} out=$RUN_DIR/fixed_reads/{sample}_R#.fixed.fastq.gz repair ow zl=7 threads=\$[SLURM_CPUS_PER_TASK - 2]; fastp -i $RUN_DIR/fixed_reads/{sample}_R1.fixed.fastq.gz -I $RUN_DIR/fixed_reads/{sample}_R2.fixed.fastq.gz --detect_adapter_for_pe -c -l 30 -p -w \$SLURM_CPUS_PER_TASK -z 7 -o $RUN_DIR/fixed_reads/{sample}_R1.fixed.trimmed.fastq.gz -O $RUN_DIR/fixed_reads/{sample}_R2.fixed.trimmed.fastq.gz -j $RUN_DIR/fixed_reads/QC/{sample}.fastp.json -h $RUN_DIR/fixed_reads/QC/{sample}.fastp.html" > $JOBNAME.cmds

# submit it as a Slurm job
echo '#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --output=%x.%j.log'"
#SBATCH --job-name=$JOBNAME
#SBATCH --cpus-per-task=$NCORES
#SBATCH --mem=${MEM}G
#SBATCH --time=$WALLTIME
#SBATCH --account=a_agri_genomics
#SBATCH --partition=general

set -Eeo pipefail
source ~/.bashrc
conda activate $CONDA_NAME
cd \$SLURM_SUBMIT_DIR
srun bash $JOBNAME.cmds" > $JOBNAME.slurm
# submit the job 
sbatch $JOBNAME.slurm

NCORES=12
MEM=64
WALLTIME="5:00:00"
JOBNAME=bwa-mem-align
# Create the bwa-mem2 commands commands to repair all read pairs.
parallel -k --dry-run --rpl "{file2} s:_R1:_R2:" --rpl "{sample} s:.+\/(.+?)_R1.+:\1:"  "ALIGN_DIR=$WORK_DIR/SNP_calling_comparison/aligned_reads/bwamem_B{2}_O{3}; mkdir -p \$ALIGN_DIR && bwa-mem2 mem -B {2} -O {3} -R \"@RG\tID:{1 sample}\tSM:{1 sample}\tLB:{1 sample}\tPU:H3HGFDSX2\tPL:ILLUMINA\tCN:AGRF\" -t \$[SLURM_CPUS_PER_TASK - 2] $GENOME.fa {1} {1 file2} | bamsormadup tmpfile=/scratch/project/adna/tmp/bamsormadup_\$(hostname) inputformat=sam threads=\$[SLURM_CPUS_PER_TASK - 2] > \$ALIGN_DIR/{1 sample}.dedup.rg.csorted.bam; unset DISPLAY; qualimap bamqc -bam \$ALIGN_DIR/{1 sample}.dedup.rg.csorted.bam --java-mem-size=4G -c -gff $GENOME.bed -outdir \$ALIGN_DIR/{1 sample}_bamqc" ::: $(find $RUN_DIR -name "*_R1.fixed.trimmed.fastq.gz") ::: 4 5 6 7 8 ::: 6 7 8 > $RUN_DIR/$JOBNAME.cmds
# submit it as a Slurm job array
sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l)%10 --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME  ~/bin/array.slurm

NCORES=12
MEM=64
WALLTIME="5:00:00"
JOBNAME=bowtie2-align
# Create the bowtie2 commands commands to repair all read pairs.
parallel -k --dry-run --rpl "{file2} s:_R1:_R2:" --rpl "{sample} s:.+/(.+?)_R1.+:\1:"  "ALIGN_DIR=$WORK_DIR/SNP_calling_comparison/aligned_reads/bt2_{2}{3}; mkdir -p \$ALIGN_DIR  && bowtie2 --{2}{3} --rg-id {1 sample} --rg 'SM:{1 sample}' --rg 'LB:{1 sample}' --rg 'PU:H3HGFDSX2' --rg 'PL:ILLUMINA' --rg 'CN:AGRF' -p \$[SLURM_CPUS_PER_TASK - 2] -x $GENOME -1 {1} -2 {1 file2} | bamsormadup tmpfile=/scratch/project/adna/tmp/bamsormadup_\$(hostname) inputformat=sam threads=\$[SLURM_CPUS_PER_TASK - 2] > \$ALIGN_DIR/{1 sample}.dedup.rg.csorted.bam; unset DISPLAY; qualimap bamqc -bam \$ALIGN_DIR/{1 sample}.dedup.rg.csorted.bam --java-mem-size=4G -c -gff $GENOME.bed -outdir \$ALIGN_DIR/{1 sample}_bamqc" ::: $(find $RUN_DIR -name "*_R1.fixed.trimmed.fastq.gz") ::: "fast" "sensitive" "very-sensitive" ::: "-local" ""  > $RUN_DIR/$JOBNAME.cmds

# submit it as a Slurm job array
sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l)%10 --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME  ~/bin/array.slurm





#  Multi-bamqc: create/copy a file describing the sample names (sample_info.txt) to the parent folder and use it in qualimap 

find `pwd` -name "*_bamqc" | gawk '{sample_name=gensub(/.+\/(.+)_bamqc/, "\\1", $1); printf "%s\t%s\n", sample_name, $1}' > multibamqc.samples
 

QUALIMAP_MULTI=$( echo "cd $RUN_DIR ; source ~/.bashrc; conda activate $CONDA_NAME; unset DISPLAY ; find `pwd` -name \"*_bamqc\" | gawk '{sample_name=gensub(/.+\/(.+)_bamqc/, \"\\\1\", \$1); printf \"%s\t%s\n\", sample_name, \$1}' > multibamqc.samples ; qualimap multi-bamqc --java-mem-size=4G -d multibamqc.samples -outformat PDF:HTML -outdir ${BATCH}_bamqc " | qsub -V -l select=1:ncpus=${NCORES}:mem=32GB,walltime=5:00:00 -N ${QUALIMAP_JOB:0:11} | egrep -o "^[0-9]+")
```



Mapping statistics were obtained with *Qualimap* v.2.2.2-dev [@okonechnikovQualimapAdvancedMultisample2016] and consolidated along with pre and post-trimming QC measures into a single, interactive report for each batch using *MultiQC* v1.19 [@ewelsMultiQCSummarizeAnalysis2016] (see [MultiQC report](raw_data/QC_11_10_2021_multiqc_report.html)).


## General information

This document was last updated at `r Sys.time()` using R Markdown (built with `r R.version.string`). The source code for this website can be found on <https://github.com/IdoBar/QX_bioinfo_analysis/tree/genome>.

Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. It is especially powerful at authoring documents and reports which include code and can execute code and use the results in the output. For more details on using R Markdown see <http://rmarkdown.rstudio.com>, [R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/) and [Rmarkdown cheatsheet](https://rstudio.github.io/cheatsheets/html/rmarkdown.html).

------------------------------------------------------------------------

## Bibliography

