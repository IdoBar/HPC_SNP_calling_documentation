@article{heldenbrandRecommendationsPerformanceOptimizations2019,
  title = {Recommendations for Performance Optimizations When Using {{GATK3}}.8 and {{GATK4}}},
  author = {Heldenbrand, Jacob R. and Baheti, Saurabh and Bockol, Matthew A. and Drucker, Travis M. and Hart, Steven N. and Hudson, Matthew E. and Iyer, Ravishankar K. and Kalmbach, Michael T. and Kendig, Katherine I. and Klee, Eric W. and Mattson, Nathan R. and Wieben, Eric D. and Wiepert, Mathieu and Wildman, Derek E. and Mainzer, Liudmila S.},
  date = {2019-11-08},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {20},
  number = {1},
  pages = {557},
  issn = {1471-2105},
  doi = {10.1186/s12859-019-3169-7},
  url = {https://doi.org/10.1186/s12859-019-3169-7},
  urldate = {2021-04-03},
  abstract = {Use of the Genome Analysis Toolkit (GATK) continues to be the standard practice in genomic variant calling in both research and the clinic. Recently the toolkit has been rapidly evolving. Significant computational performance improvements have been introduced in GATK3.8 through collaboration with Intel in 2017. The first release of GATK4 in early 2018 revealed rewrites in the code base, as the stepping stone toward a Spark implementation. As the software continues to be a moving target for optimal deployment in highly productive environments, we present a detailed analysis of these improvements, to help the community stay abreast with changes in performance.},
  keywords = {Best practices,Cluster computing,Computational performance,GATK,Genomic variant calling,Parallelization}
}

@incollection{vanderauweraFastQDataHighConfidence2002,
  title = {From {{FastQ Data}} to {{High-Confidence Variant Calls}}: {{The Genome Analysis Toolkit Best Practices Pipeline}}},
  shorttitle = {From {{FastQ Data}} to {{High-Confidence Variant Calls}}},
  booktitle = {Current {{Protocols}} in {{Bioinformatics}}},
  author = {Van der Auwera, Geraldine A. and Carneiro, Mauricio O. and Hartl, Christopher and Poplin, Ryan and family=Angel, given=Guillermo, prefix=del, useprefix=true and Levy-Moonshine, Ami and Jordan, Tadeusz and Shakir, Khalid and Roazen, David and Thibault, Joel and Banks, Eric and Garimella, Kiran V. and Altshuler, David and Gabriel, Stacey and DePristo, Mark A.},
  date = {2002},
  publisher = {John Wiley \& Sons, Inc.},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/0471250953.bi1110s43/abstract},
  urldate = {2015-04-03},
  abstract = {This unit describes how to use BWA and the Genome Analysis Toolkit (GATK) to map genome sequencing data to a reference and produce high-quality variant calls that can be used in downstream analyses. The complete workflow includes the core NGS data-processing steps that are necessary to make the raw data suitable for analysis by the GATK, as well as the key methods involved in variant discovery using the GATK. Curr. Protoc. Bioinform. 43:11.10.1-11.10.33. \copyright{} 2013 by John Wiley \& Sons, Inc.},
  isbn = {978-0-471-25095-1},
  langid = {english},
  keywords = {exome,genotyping,NGS,variant detection,WGS}
}


@online{clearyComparingVariantCall2015,
  title = {Comparing {{Variant Call Files}} for {{Performance Benchmarking}} of {{Next-Generation Sequencing Variant Calling Pipelines}}},
  author = {Cleary, John G. and Braithwaite, Ross and Gaastra, Kurt and Hilbush, Brian S. and Inglis, Stuart and Irvine, Sean A. and Jackson, Alan and Littin, Richard and Rathod, Mehul and Ware, David and Zook, Justin M. and Trigg, Len and Vega, Francisco M. De La},
  date = {2015-08-03},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {023754},
  doi = {10.1101/023754},
  abstract = {Summary To evaluate and compare the performance of variant calling methods and their confidence scores, comparisons between a test call set and a ``gold standard'' need to be carried out. Unfortunately, these comparisons are not straightforward with the current Variant Call Files (VCF), which are the standard output of most variant calling algorithms for high-throughput sequencing data. Comparisons of VCFs are often confounded by the different representations of indels, MNPs, and combinations thereof with SNVs in complex regions of the genome, resulting in misleading results. A variant caller is inherently a classification method designed to score putative variants with confidence scores that could permit controlling the rate of false positives (FP) or false negatives (FN) for a given application. Receiver operator curves (ROC) and the area under the ROC (AUC) are efficient metrics to evaluate a test call set versus a gold standard. However, in the case of VCF data this also requires a special accounting to deal with discrepant representations. We developed a novel algorithm for comparing variant call sets that deals with complex call representation discrepancies and through a dynamic programing method that minimizes false positives and negatives globally across the entire call sets for accurate performance evaluation of VCFs. Availability RTG Tools is implemented as a multithreaded Java application and source code is available under BSD license at: https://github.com/RealTimeGenomics/rtg-tools Contact len\{at\}realtimegenomics.com},
  langid = {english},
  pubstate = {prepublished}
}

@article{liComparingGenomicVariant2023,
  title = {Comparing Genomic Variant Identification Protocols for {{{\mkbibemph{Candida auris}}}}},
  author = {Li, Xiao and Mu\~noz, Jos\'e F. and Gade, Lalitha and Argimon, Silvia and Bougnoux, Marie-Elisabeth and Bowers, Jolene R. and Chow, Nancy A. and Cuesta, Isabel and Farrer, Rhys A. and Maufrais, Corinne and Monroy-Nieto, Juan and Pradhan, Dibyabhaba and Uehling, Jessie and Vu, Duong and Yeats, Corin A. and Aanensen, David M. and family=Enfert, given=Christophe, prefix=d', useprefix=true and Engelthaler, David M. and Eyre, David W. and Fisher, Matthew C. and Hagen, Ferry and Meyer, Wieland and Singh, Gagandeep and Alastruey-Izquierdo, Ana and Litvintseva, Anastasia P. and Cuomo, Christina A.},
  date = {2023-04-12},
  journaltitle = {Microbial Genomics},
  shortjournal = {Microb. Genom.},
  volume = {9},
  number = {4},
  pages = {000979},
  publisher = {Microbiology Society,},
  issn = {2057-5858},
  doi = {10.1099/mgen.0.000979},
  abstract = {Genomic analyses are widely applied to epidemiological, population genetic and experimental studies of pathogenic fungi. A wide range of methods are employed to carry out these analyses, typically without including controls that gauge the accuracy of variant prediction. The importance of tracking outbreaks at a global scale has raised the urgency of establishing high-accuracy pipelines that generate consistent results between research groups. To evaluate currently employed methods for whole-genome variant detection and elaborate best practices for fungal pathogens, we compared how 14 independent variant calling pipelines performed across 35               Candida auris               isolates from 4 distinct clades and evaluated the performance of variant calling, single-nucleotide polymorphism (SNP) counts and phylogenetic inference results. Although these pipelines used different variant callers and filtering criteria, we found high overall agreement of SNPs from each pipeline. This concordance correlated with site quality, as SNPs discovered by a few pipelines tended to show lower mapping quality scores and depth of coverage than those recovered by all pipelines. We observed that the major differences between pipelines were due to variation in read trimming strategies, SNP calling methods and parameters, and downstream filtration criteria. We calculated specificity and sensitivity for each pipeline by aligning three isolates with chromosomal level assemblies and found that the GATK-based pipelines were well balanced between these metrics. Selection of trimming methods had a greater impact on SAMtools-based pipelines than those using GATK. Phylogenetic trees inferred by each pipeline showed high consistency at the clade level, but there was more variability between isolates from a single outbreak, with pipelines that used more stringent cutoffs having lower resolution. This project generated two truth datasets useful for routine benchmarking of               C. auris               variant calling, a consensus VCF of genotypes discovered by 10 or more pipelines across these 35 diverse isolates and variants for 2 samples identified from whole-genome alignments. This study provides a foundation for evaluating SNP calling pipelines and developing best practices for future fungal genomic studies.},
  langid = {american}
}

@article{rudenUsingDrosophilaMelanogaster2012,
  title = {Using {{{\emph{Drosophila}}}}{\emph{ Melanogaster}} as a Model for Genotoxic Chemical Mutational Studies with a New Program, {{SnpSift}}},
  author = {Ruden, Douglas Mark and Cingolani, Pablo and Patel, Viral M. and Coon, Melissa and Nguyen, Tung and Land, Susan J. and Lu, Xiangyi},
  date = {2012},
  journaltitle = {Toxicogenomics},
  shortjournal = {Drosophila melanogaster},
  volume = {3},
  pages = {35},
  doi = {10.3389/fgene.2012.00035},
  url = {http://journal.frontiersin.org/article/10.3389/fgene.2012.00035/full},
  urldate = {2016-11-04},
  abstract = {This paper describes a new program SnpSift for filtering differential DNA sequence variants between two or more experimental genomes after genotoxic chemical exposure. Here, we illustrate how SnpSift can be used to identify candidate phenotype-relevant variants including single nucleotide polymorphisms, multiple nucleotide polymorphisms, insertions, and deletions (InDels) in mutant strains isolated from genome-wide chemical mutagenesis of Drosophila melanogaster. First, the genomes of two independently isolated mutant fly strains that are allelic for a novel recessive male-sterile locus generated by genotoxic chemical exposure were sequenced using the Illumina next-generation DNA sequencer to obtain 20- to 29-fold coverage of the euchromatic sequences. The sequencing reads were processed and variants were called using standard bioinformatic tools. Next, SnpEff was used to annotate all sequence variants and their potential mutational effects on associated genes. Then, SnpSift was used to filter and select differential variants that potentially disrupt a common gene in the two allelic mutant strains. The potential causative DNA lesions were partially validated by capillary sequencing of polymerase chain reaction-amplified DNA in the genetic interval as defined by meiotic mapping and deletions that remove defined regions of the chromosome. Of the five candidate genes located in the genetic interval, the Pka-like gene CG12069 was found to carry a separate pre-mature stop codon mutation in each of the two allelic mutants whereas the other four candidate genes within the interval have wild-type sequences. The Pka-like gene is therefore a strong candidate gene for the male-sterile locus. These results demonstrate that combining SnpEff and SnpSift can expedite the identification of candidate phenotype-causative mutations in chemically mutagenized Drosophila strains. This technique can also be used to characterize the variety of mutations generated by genotoxic chemicals.},
  keywords = {Drosophila melanogaster,next-generation DNA sequencing,personal genomes,whole-genome SNP analysis}
}

@article{chenUltrafastOnepassFASTQ2023,
  title = {Ultrafast One-Pass {{FASTQ}} Data Preprocessing, Quality Control, and Deduplication Using Fastp},
  author = {Chen, Shifu},
  date = {2023-05},
  journaltitle = {iMeta},
  shortjournal = {iMeta},
  volume = {2},
  number = {2},
  pages = {e107},
  issn = {2770-596X, 2770-596X},
  doi = {10.1002/imt2.107},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/imt2.107},
  urldate = {2023-07-06},
  abstract = {A large amount of sequencing data is generated and processed every day with the continuous evolution of sequencing technology and the expansion of sequencing applications. One consequence of such sequencing data explosion is the increasing cost and complexity of data processing. The preprocessing of FASTQ data, which means removing adapter contamination, filtering low-quality reads, and correcting wrongly represented bases, is an indispensable but resource intensive part of sequencing data analysis. Therefore, although a lot of software applications have been developed to solve this problem, bioinformatics scientists and engineers are still pursuing faster, simpler, and more energy-efficient software. Several years ago, the author developed fastp, which is an ultrafast all-in-one FASTQ data preprocessor with many modern features. This software has been approved by many bioinformatics users and has been continuously maintained and updated. Since the first publication on fastp, it has been greatly improved, making it even faster and more powerful. For instance, the duplication evaluation module has been improved, and a new deduplication module has been added. This study aimed to introduce the new features of fastp and demonstrate how it was designed and implemented.},
  langid = {american},
  keywords = {adapter,duplication,FASTQ,filtering,preprocessing,quality control}
}

@unpublished{garrisonHaplotypebasedVariantDetection2012a,
  title = {Haplotype-Based Variant Detection from Short-Read Sequencing},
  author = {Garrison, Erik and Marth, Gabor},
  date = {2012-07-20},
  eprint = {1207.3907},
  eprinttype = {arxiv},
  eprintclass = {q-bio},
  url = {http://arxiv.org/abs/1207.3907},
  urldate = {2021-01-12},
  abstract = {The direct detection of haplotypes from short-read DNA sequencing data requires changes to existing small-variant detection methods. Here, we develop a Bayesian statistical framework which is capable of modeling multiallelic loci in sets of individuals with non-uniform copy number. We then describe our implementation of this framework in a haplotype-based variant detector, FreeBayes.},
  keywords = {Quantitative Biology - Genomics,Quantitative Biology - Quantitative Methods}
}

@article{danecekVariantCallFormat2011a,
  title = {The Variant Call Format and {{VCFtools}}},
  author = {Danecek, Petr and Auton, Adam and Abecasis, Goncalo and Albers, Cornelis A. and Banks, Eric and DePristo, Mark A. and Handsaker, Robert E. and Lunter, Gerton and Marth, Gabor T. and Sherry, Stephen T. and McVean, Gilean and Durbin, Richard and {1000 Genomes Project Analysis Group}},
  date = {2011-08-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {27},
  number = {15},
  pages = {2156--2158},
  issn = {1367-4811, 1367-4803},
  doi = {10.1093/bioinformatics/btr330},
  url = {https://doi.org/10.1093/bioinformatics/btr330},
  urldate = {2023-06-27},
  abstract = {Abstract             Summary: The variant call format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations. VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome. The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project. VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing and also provides a general Perl API.             Availability: ~http://vcftools.sourceforge.net             Contact: ~rd@sanger.ac.uk},
  langid = {american}
}

@article{barbitoffSystematicBenchmarkStateoftheart2022,
  title = {Systematic Benchmark of State-of-the-Art Variant Calling Pipelines Identifies Major Factors Affecting Accuracy of Coding Sequence Variant Discovery},
  author = {Barbitoff, Yury A. and Abasov, Ruslan and Tvorogova, Varvara E. and Glotov, Andrey S. and Predeus, Alexander V.},
  date = {2022-12},
  journaltitle = {BMC Genomics},
  shortjournal = {Bmc Genomics},
  volume = {23},
  number = {1},
  pages = {155},
  issn = {1471-2164},
  doi = {10.1186/s12864-022-08365-3},
  url = {https://doi.org/10.1186/s12864-022-08365-3},
  urldate = {2023-05-24},
  abstract = {Abstract                            Background               Accurate variant detection in the coding regions of the human genome is a key requirement for molecular diagnostics of Mendelian disorders. Efficiency of variant discovery from next-generation sequencing (NGS) data depends on multiple factors, including reproducible coverage biases of NGS methods and the performance of read alignment and variant calling software. Although variant caller benchmarks are published constantly, no previous publications have leveraged the full extent of available gold standard whole-genome (WGS) and whole-exome (WES) sequencing datasets.                                         Results               In this work, we systematically evaluated the performance of 4 popular short read aligners (Bowtie2, BWA, Isaac, and Novoalign) and 9 novel and well-established variant calling and filtering methods (Clair3, DeepVariant, Octopus, GATK, FreeBayes, and Strelka2) using a set of 14 b},
  langid = {american}
}
@article{danecekBCFtoolsCsqHaplotypeaware2017,
  title = {{{BCFtools}}/Csq: Haplotype-Aware Variant Consequences},
  shorttitle = {{{BCFtools}}/Csq},
  author = {Danecek, Petr and McCarthy, Shane A},
  editor = {Birol, Inanc},
  date = {2017-07-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {33},
  number = {13},
  eprint = {28205675},
  eprinttype = {pmid},
  pages = {2037--2039},
  issn = {1367-4803, 1367-4811},
  doi = {10.1093/bioinformatics/btx100},
  url = {https://academic.oup.com/bioinformatics/article/33/13/2037/3000373},
  abstract = {Abstract                            Motivation               Prediction of functional variant consequences is an important part of sequencing pipelines, allowing the categorization and prioritization of genetic variants for follow up analysis. However, current predictors analyze variants as isolated events, which can lead to incorrect predictions when adjacent variants alter the same codon, or when a frame-shifting indel is followed by a frame-restoring indel. Exploiting known haplotype information when making consequence predictions can resolve these issues.                                         Results               BCFtools/csq is a fast program for haplotype-aware consequence calling which can take into account known phase. Consequence predictions are changed for 501 of 5019 compound variants found in the 81.7M variants in the 1000 Genomes Project data, with an average of 139 compound variants per haplotype. Predictions match existing tools when run in localized mode, but the program is an order of magnitude faster and requires an order of magnitude less memory.                                         Availability and Implementation               The program is freely available for commercial and non-commercial use in the BCFtools package which is available for download from http://samtools.github.io/bcftools.                                         Supplementary information               Supplementary data are available at Bioinformatics online.},
  langid = {american},
  pmcid = {PMC5870570},
  keywords = {Algorithms,Genetic Variation,{Genome, Human},Genomics,Haplotypes,Humans,INDEL Mutation,{Sequence Analysis, DNA},Software}
}

@article{danecekTwelveYearsSAMtools2021,
  title = {Twelve Years of {{SAMtools}} and {{BCFtools}}},
  author = {Danecek, Petr and Bonfield, James K and Liddle, Jennifer and Marshall, John and Ohan, Valeriu and Pollard, Martin O and Whitwham, Andrew and Keane, Thomas and McCarthy, Shane A and Davies, Robert M and Li, Heng},
  date = {2021-01-29},
  journaltitle = {GigaScience},
  shortjournal = {GigaScience},
  volume = {10},
  number = {2},
  eprint = {33590861},
  eprinttype = {pmid},
  pages = {giab008},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giab008},
  url = {https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giab008/6137722},
  abstract = {Abstract                            Background               SAMtools and BCFtools are widely used programs for processing and analysing high-throughput sequencing data. They include tools for file format conversion and manipulation, sorting, querying, statistics, variant calling, and effect analysis amongst other methods.                                         Findings               The first version appeared online 12 years ago and has been maintained and further developed ever since, with many new features and improvements added over the years. The SAMtools and BCFtools packages represent a unique collection of tools that have been used in numerous other software projects and countless genomic pipelines.                                         Conclusion               Both SAMtools and BCFtools are freely available on GitHub under the permissive MIT licence, free for both non-commercial and commercial use. Both packages have been installed \&gt;1 million times via Bioconda. The source code and documentation are available from https://www.htslib.org.},
  langid = {american},
  pmcid = {PMC7931819},
  keywords = {bcftools,data analysis,Genome,Genomics,High-Throughput Nucleotide Sequencing,high-throughput sequencing,next generation sequencing,samtools,Software,variant calling}
}

@article{danecekVariantCallFormat2011a,
  title = {The Variant Call Format and {{VCFtools}}},
  author = {Danecek, Petr and Auton, Adam and Abecasis, Goncalo and Albers, Cornelis A. and Banks, Eric and DePristo, Mark A. and Handsaker, Robert E. and Lunter, Gerton and Marth, Gabor T. and Sherry, Stephen T. and McVean, Gilean and Durbin, Richard and {1000 Genomes Project Analysis Group}},
  date = {2011-08-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {27},
  number = {15},
  pages = {2156--2158},
  issn = {1367-4811, 1367-4803},
  doi = {10.1093/bioinformatics/btr330},
  url = {https://doi.org/10.1093/bioinformatics/btr330},
  urldate = {2023-06-27},
  abstract = {Abstract             Summary: The variant call format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations. VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome. The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project. VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing and also provides a general Perl API.             Availability: ~http://vcftools.sourceforge.net             Contact: ~rd@sanger.ac.uk},
  langid = {american}
}
@article{ewelsMultiQCSummarizeAnalysis2016,
  title = {{{MultiQC}}: Summarize Analysis Results for Multiple Tools and Samples in a Single Report},
  shorttitle = {{{MultiQC}}},
  author = {Ewels, Philip and Magnusson, MC%ns and Lundin, Sverker and KC$ller, Max},
  date = {2016-10-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {32},
  number = {19},
  eprint = {27312411},
  eprinttype = {pmid},
  pages = {3047--3048},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btw354},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5039924/},
  urldate = {2017-07-19},
  abstract = {Motivation: Fast and accurate quality control is essential for studies involving next-generation sequencing data. Whilst numerous tools exist to quantify QC metrics, there is no common approach to flexibly integrate these across tools and large sample sets. Assessing analysis results across an entire project can be time consuming and error prone; batch effects and outlier samples can easily be missed in the early stages of analysis., Results: We present MultiQC, a tool to create a single report visualising output from multiple tools across many samples, enabling global trends and biases to be quickly identified. MultiQC can plot data from many common bioinformatics tools and is built to allow easy extension and customization., Availability and implementation: MultiQC is available with an GNU GPLv3 license on GitHub, the Python Package Index and Bioconda. Documentation and example reports are available at  http://multiqc.info, Contact: phil.ewels@scilifelab.se},
  pmcid = {PMC5039924},
  annotation = {00021}
}

@article{garciaSarekPortableWorkflow2020,
  title = {Sarek: {{A}} Portable Workflow for Whole-Genome Sequencing Analysis of Germline and Somatic Variants},
  shorttitle = {Sarek},
  author = {Garcia, Maxime and Juhos, Szilveszter and Larsson, Malin and Olason, Pall I. and Martin, Marcel and Eisfeldt, Jesper and DiLorenzo, Sebastian and Sandgren, Johanna and StC%hl, Teresita DC-az De and Ewels, Philip and Wirta, Valtteri and NistC)r, Monica and KC$ller, Max and Nystedt, BjC6rn},
  date = {2020-09-04},
  journaltitle = {F1000Research},
  volume = {9},
  number = {63},
  doi = {10.12688/f1000research.16665.2},
  url = {https://f1000research.com/articles/9-63},
  urldate = {2022-07-04},
  abstract = {Whole-genome sequencing (WGS) is a fundamental technology for research to advance precision medicine, but the limited availability of portable and user-friendly workflows for WGS analyses poses a major challenge for many research groups and hampers scientific progress. Here we present Sarek, an open-source workflow to detect germline variants and somatic mutations based on sequencing data from WGS, whole-exome sequencing (WES), or gene panels. Sarek features (i) easy installation, (ii) robust portability across different computer environments, (iii) comprehensive documentation, (iv) transparent and easy-to-read code, and (v) extensive quality metrics reporting. Sarek is implemented in the Nextflow workflow language and supports both Docker and Singularity containers as well as Conda environments, making it ideal for easy deployment on any POSIX-compatible computers and cloud compute environments. Sarek follows the GATK best-practice recommendations for read alignment and pre-processing, and includes a wide range of software for the identification and annotation of germline and somatic single-nucleotide variants, insertion and deletion variants, structural variants, tumour sample purity, and variations in ploidy and copy number. Sarek offers easy, efficient, and reproducible WGS analyses, and can readily be used both as a production workflow at sequencing facilities and as a powerful stand-alone tool for individual research groups. The Sarek source code, documentation and installation instructions are freely available at https://github.com/nf-core/sarek and at https://nf-co.re/sarek/ .},
  langid = {english},
  keywords = {Analysis workflow,Cancer,Germline variants,Somatic variants,Whole Genome Sequencing}
}
@article{liStatisticalFrameworkSNP2011,
  title = {A Statistical Framework for {{SNP}} Calling, Mutation Discovery, Association Mapping and Population Genetical Parameter Estimation from Sequencing Data},
  author = {Li, Heng},
  date = {2011-11-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {27},
  number = {21},
  eprint = {21903627},
  eprinttype = {pmid},
  pages = {2987--2993},
  issn = {1367-4811, 1367-4803},
  doi = {10.1093/bioinformatics/btr509},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3198575/},
  urldate = {2023-06-27},
  abstract = {Abstract             Motivation: Most existing methods for DNA sequence analysis rely on accurate sequences or genotypes. However, in applications of the next-generation sequencing (NGS), accurate genotypes may not be easily obtained (e.g. multi-sample low-coverage sequencing or somatic mutation discovery). These applications press for the development of new methods for analyzing sequence data with uncertainty.             Results: We present a statistical framework for calling SNPs, discovering somatic mutations, inferring population genetical parameters and performing association tests directly based on sequencing data without explicit genotyping or linkage-based imputation. On real data, we demonstrate that our method achieves comparable accuracy to alternative methods for estimating site allele count, for inferring allele frequency spectrum and for association mapping. We also highlight the necessity of using symmetric datasets for finding somatic mutations and confirm that for discovering rare events, mismapping is frequently the leading source of errors.             Availability: ~http://samtools.sourceforge.net             Contact: ~hengli@broadinstitute.org},
  langid = {american},
  pmcid = {PMC3198575}
}

@article{okonechnikovQualimapAdvancedMultisample2016,
  title = {Qualimap 2: Advanced Multi-Sample Quality Control for High-Throughput Sequencing Data},
  shorttitle = {Qualimap 2},
  author = {Okonechnikov, Konstantin and Conesa, Ana and GarcC-a-Alcalde, Fernando},
  date = {2016-01-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {32},
  number = {2},
  pages = {292--294},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btv566},
  url = {https://academic.oup.com/bioinformatics/article/32/2/292/1744356/Qualimap-2-advanced-multi-sample-quality-control},
  urldate = {2017-07-28},
  abstract = {Motivation: Detection of random errors and systematic biases is a crucial step of a robust pipeline for processing high-throughput sequencing (HTS) data. Bioinformatics software tools capable of performing this task are available, either for general analysis of HTS data or targeted to a specific sequencing technology. However, most of the existing QC instruments only allow processing of one sample at a time.Results: Qualimap 2 represents a next step in the QC analysis of HTS data. Along with comprehensive single-sample analysis of alignment data, it includes new modes that allow simultaneous processing and comparison of multiple samples. As with the first version, the new features are available via both graphical and command line interface. Additionally, it includes a large number of improvements proposed by the user community.Availability and implementation: The implementation of the software along with documentation is freely available at http://www.qualimap.org.Contact:meyer@mpiib-berlin.mpg.deSupplementary information:Supplementary data are available at Bioinformatics online.},
  annotation = {00050}
}
@article{tischlerBiobambamToolsRead2014,
  title = {Biobambam: Tools for Read Pair Collation Based Algorithms on {{BAM}} Files},
  shorttitle = {Biobambam},
  author = {Tischler, German and Leonard, Steven},
  date = {2014-06-20},
  journaltitle = {Source Code for Biology and Medicine},
  shortjournal = {Source Code for Biology and Medicine},
  volume = {9},
  number = {1},
  pages = {13},
  issn = {1751-0473},
  doi = {10.1186/1751-0473-9-13},
  url = {https://doi.org/10.1186/1751-0473-9-13},
  urldate = {2020-11-25},
  abstract = {Sequence alignment data is often ordered by coordinate (id of the reference sequence plus position on the sequence where the fragment was mapped) when stored in BAM files, as this simplifies the extraction of variants between the mapped data and the reference or of variants within the mapped data. In this order paired reads are usually separated in the file, which complicates some other applications like duplicate marking or conversion to the FastQ format which require to access the full information of the pairs.}
}
@inproceedings{vasimuddinEfficientArchitectureAwareAcceleration2019,
  title = {Efficient {{Architecture-Aware Acceleration}} of {{BWA-MEM}} for {{Multicore Systems}}},
  booktitle = {2019 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {family=Vasimuddin, given=Md., given-i={{Md}} and Misra, Sanchit and Li, Heng and Aluru, Srinivas},
  date = {2019-05},
  pages = {314--324},
  issn = {1530-2075},
  doi = {10.1109/IPDPS.2019.00041},
  abstract = {Innovations in Next-Generation Sequencing are enabling generation of DNA sequence data at ever faster rates and at very low cost. For example, the Illumina NovaSeq 6000 sequencer can generate 6 Terabases of data in less than two days, sequencing nearly 20 Billion short DNA fragments called reads at the low cost of \$1000 per human genome. Large sequencing centers typically employ hundreds of such systems. Such highthroughput and low-cost generation of data underscores the need for commensurate acceleration in downstream computational analysis of the sequencing data. A fundamental step in downstream analysis is mapping of the reads to a long reference DNA sequence, such as a reference human genome. Sequence mapping is a compute-intensive step that accounts for more than 30\% of the overall time of the GATK (Genome Analysis ToolKit) best practices workflow. BWA-MEM is one of the most widely used tools for sequence mapping and has tens of thousands of users. In this work, we focus on accelerating BWA-MEM through an efficient architecture aware implementation, while maintaining identical output. The volume of data requires distributed computing and is usually processed on clusters or cloud deployments with multicore processors usually being the platform of choice. Since the application can be easily parallelized across multiple sockets (even across distributed memory systems) by simply distributing the reads equally, we focus on performance improvements on a single socket multicore processor. BWA-MEM run time is dominated by three kernels, collectively responsible for more than 85\% of the overall compute time. We improved the performance of the three kernels by 1) using techniques to improve cache reuse, 2) simplifying the algorithms, 3) replacing many small memory allocations with a few large contiguous ones to improve hardware prefetching of data, 4) software prefetching of data, and 5) utilization of SIMD wherever applicable and massive reorganization of the source code to enable these improvements. As a result, we achieved nearly 2x, 183x, and 8x speedups on the three kernels, respectively, resulting in up to 3.5x and 2.4x speedups on end-to-end compute time over the original BWA-MEM on single thread and single socket of Intel Xeon Skylake processor. To the best of our knowledge, this is the highest reported speedup over BWA-MEM (running on a single CPU) while using a single CPU or a single CPU-single GPGPU/FPGA combination.},
  eventtitle = {2019 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  keywords = {acceleration,Acceleration,architecture aware implementation,architecture-aware acceleration,bioinformatics,Bioinformatics,biology computing,BWA,BWA-MEM,cache storage,commensurate acceleration,compute-intensive step,coprocessors,data underscores,distributed memory systems,DNA,DNA sequence data,downstream analysis,downstream computational analysis,end-to-end compute time,field programmable gate arrays,GATK best practices workflow,genetics,genome analysis toolkit,genomics,Genomics,Illumina NovaSeq 6000 sequencer,Kernel,low-cost generation,molecular biophysics,multi-threading,multicore CPU,Multicore processing,multicore processors,multicore systems,multiprocessing systems,next-generation sequencing,parallel algorithms,parallel architectures,parallel processing,Program processors,reference human genome,sequence mapping,sequencing centers,sequencing data,Sequential analysis,single socket multicore processor,storage management}
}
